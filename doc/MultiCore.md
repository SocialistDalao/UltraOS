## MultiCore

目前来看，多核的探索是成功的，支持是失败的。截至6月20号，一切安好，所有的进程都可以多核运行，截至7月20号，事情不妙，出现了进程逃逸和分裂，截至8月18号，灰飞烟灭，多核能启动，但是运行不起来。

随着系统调用和内核功能的急剧增加以及架构的复杂化技巧化，多核的支持突然变得极为棘手，但是很明显，我们的精力已经不够了。即使在现在，多核还是我们一个非常想要达到的目标，实际上一开始我们的目标就是所有的代码都应该要支持多核，因为未来绝对是多核的，而多核的编程模式和单核注定不相同，这也是我们从一开始的编写就考虑多核的根本原因。但是最后还是错付了，累了。

### RustSBI

双核必须先对RustSBI进行修改。RustSBI实际上已经支持了双核，在一开始的配置之下，根据设定的最多支持核心数（实际上是hart数，但是现在单核对应一个hart，可以先看作核心数），m态获取到hartid，取出与最多支持核心数比较，多余的直接进入设定的死循环程序抛弃。但是这并不代表支持的核心就可以被唤醒。

#### 初始化

实际上，需要让一个核先做初始化，而非一开始就释放所有的核心运行。在RustSBI，要设置串口等硬件，将其配置至正确的状态，使其能够正确运行。这个时候，RustSBI会将非零号核心进入循环等待，反复读取中断值等待中断的到来（软中断）。这个中断是IPI，核间通信中断。。但是看了xv6的移植解决方案之后，看到其使用的方法为在s态ecall IPI，选定唤醒。这个时候，其他核就会唤醒，执行之后的操作，这个时候主核已经做好了所有的工作，因此该核心只需要关心自己核内部的初始化，然后进入os kernel。

#### 告知操作系统hartid

获取当前核心的id必须要在m态才能获取，反复的从s态陷入显然不符合设计，因此要从RustSBI将其传入s态。目前采用的办法是将其保存至tp寄存器，进入内核之后再获取。

### 内核UltraOS

#### 初始化

同样的，操作系统也要完成初始化，包括对进程、内存分配、页表等部分的初始化。当然，我们也要对核心本身做一部分初始化，因为有些数据不是共有的，而是每个核心所独占的。在主核初始化之后，要唤醒副核做自己的初始化。唤醒的时候也类似RustSBI，副核loop读取某个全局变量，主核初始化完成之后通过写这个变量使得副核跳出loop做自己的初始化。

##### 处理器管理器

目前主核要做两个初始化，一是Processor Manager，每个核都要拥有一个，并且据此来保存当前运行的PCB，并且实现进程调度。

##### 页表

每个处理器核的内核页表相同，但这并不意味着不需要设置。我们需要主动将页表挂载到CSR上，才能够正确访问内核栈等非一一映射的地址空间。注意还要使用sfence.vma清TLB。

##### 中断

开启trap（指内核中断，用户中断会在返回用户前才开启），时钟等中断。但是这里我们没有关闭副核的一些外部中断，当出现一些外部中断的时候也许会导致多核同时响应，现在我们还没有遇到问题，暂时不考虑。

#### 内核运行时

内核多核运行的时候，对于内存一致性的要求更高，出现的错误也是千奇百怪，这部分内容我们放在了MemoryConsistency.md中进行叙述。

##### TLB Shootdown（暂无必要）

现在，我们实现的地址空间中，内核页表为共享页表。所有的进程的内核页表映射都来自于一个共享的基页表，那么对于共享页表的修改应该要被全局页表所感知。而恰好，我们有两个部分需要全局修改，一个是kmmap一个是内核栈。kmmap仅限于当前进程使用，并且即用即放，因此无需同步。但是对于内核栈，则需要通知。对于同一的虚拟地址，两个核可能因为内核栈的释放和获取先后进行，而使得两个核的映射不一致，当然也可以推至多核的情形。但是，我们在切换进程调度的时候会切换页表，与此同时，我们使用了sfence.vma指令，这意味着每个进程被获取的时候TLB都会与内存同步，这就**不会出现上述问题**。

但是我们不确定之后会不会遇到这个问题，TLB Shootdown只在多核共用同一个页表的时候出现。下面给出TLB Shootdown的处理方法。

我们首先要进行TLB同步，不仅要将Cache中的数据（页表修改结果会暂存至Cache）同步至内存，还需要通知所有的核都刷新TLB，这个操作需要通过核间通信完成。具体步骤如下：

- fence：将Cache同步至内存
- 发送IPI（核间中断）给所有核
- sfence.VMA：刷新TLB

通常，这个操作由SBI来完成较为适合，因为不用在内核加入额外保存上下文等操作（移至SBI）。

##### 调度队列hash（尚未实现）

在进程调度的时候，每次都需要争抢对应的锁，我们为了减少锁的争用，可以将调度器分成多个队列，用hart id进行hash先寻找对应的调度器队列，用pid进行hash入队。这样，在进程充足的时候，可以不用争用锁。进程不充足的时候，就需要依次遍历所有队列，来与其他核争抢锁。

##### hartid保持（尚未实现）

目前，在普通的用户测试程序面前，我们不会使用到tp寄存器，但是在复杂程序运行的时候则会。目前我们将hartid存放在tp寄存器，而要想获取到核心的id必须访问mhartid寄存器，这意味着我们必须陷入到m态，而获取hartid的操作应该是非常频繁的（调度等），而且随着功能的增多以及优化会变得更为关键（核hash避免锁竞争）。

因此，我们必须想办法在内核保存hartid。但是设计一个全局变量是不可行的，因为这对每个核心都是等价的。因此，我们选择的方法是延续初始的tp，保持该数值能够一直在该核的掌控之中。因此，我们将其放在了PCB的trap context中，但是它保存的不是用户的上下文，而是内核的上下文中的tp（hartid）。这样，每当trap return时，我们就将tp保存下来，而trap in的时候就恢复。这样，我们就能够让hartid一直保存在内核的范围中。注意，这里隐藏了默认内核程序不会修改tp的原理。

同时，我们可以看到，在PCB中存放tp还有一个好处，那就是即使出现了进程在核间切换运行，核也能够知道通过调度得到的进程上一个进程在哪个核运行。以此，我们可以利用内存的弱一致性原理，只在跨核的时候进行内存一致性保证的操作，减少开销。





#### 内核相关的数据位置和使用的探讨

##### lazy_static

lazy_static按照现在的理解，是将数据存在堆中，但是其指针存在于全局变量之中。同时，对于其本体，一次也只会分配一个，也就是说使用lazy_alloc不会因为多核执行多次内核代码而分配多次数据，所有的核都共享这一个全局变量。如果我们要分配多个，那就要分配数组，或者是向量。现在多核最多两个，因此对于processor manager我们直接分配两个manager组成的数组，每次根据核心id来访问对应的manager。

还有就是，只能给lazy_static初始化一个不可变变量，如果要初始化一个可变变量，就要用线程安全的Arc。

##### Arc
Arc
Arc 是原子引用计数，是 Rc 的多线程版本。Arc 通过 std::sync::Arc 引入。

它的特点：

- Arc 可跨线程传递，用于跨线程共享一个对象；
- 用 Arc 包裹起来的类型对象，对可变性没有要求；
- 一旦最后一个拥有者消失，则资源会被自动回收，这个生命周期是在编译期就确定下来的；
- Arc 实际上是一个指针，它不影响包裹对象的方法调用形式（即不存在先解开包裹再调用值这一说）；
- Arc 对于多线程的共享状态几乎是必须的（减少复制，提高性能）。


### 问题

现在在forktest2的测试中偶见卡死，初步判断应该是出现了死锁。同时我们发现，当fork和sleep搭配的时候，就会出现包括操作系统卡死、僵尸进程未成功回收等情况。当我们将其放在rCoreTutorial的ch5分支（抛去了我们所有做的工作，同时还尽可能的简化了内核）上，还是在fork于sleep相关的地方发生了问题。报出的错误是instruction page fault，基本上可以猜测到是因为pc去向未定义地址处导致读出非预期数据，出发指令未识别错误。

对于卡死，我们发现基本都是在地址0xfffffffffffff04e附近卡死的，初步判定是虚地址，位于trampoline。

对于instruction page fault，gdb在trap from kernel设置断点，发现发生在pc=0，意味着程序在某时跳转到了0处，很有可能是ra变成了0.

#### 死锁（已解决）

在程序压力fork和sleep压力非常大的时候，会经常出现exit和yield，使得双核操作系统会经常同时处于内核态，并且频繁的同时进行进程调度与切换相关操作。其中出现问题的关键是：当双核同时处理进程exit时，并且其中一个是另一个的父进程，就会出现死锁，死锁出现在exit_current_and_run_next程序之中。该程序会以以下程序顺序获得锁：
- 获取当前进程的锁，以取出当前进程的孩子进程的指针
- 获取初始进程的锁，以准备将孩子进程挂载上去
- 依次获取孩子的锁，挂载

这样，当双核同时处理进程exit时，并且其中一个是另一个的父进程的时候，父进程获得当前进程和初始进程的锁，子进程获得了当前进程的锁。父进程想要获得孩子的锁，而孩子又想要获得初始进程的锁，而他们都被对方所获取，这就出现了死锁。


#### 程序跑飞而导致Instruction Page Fault（已解决）

这个死锁的直接结果就是在内核因为ra异常，程序运行在了没有设定的地址范围内。在排查之后，发现间接原因是task_cx被修改了。然后，使用gdb抓取出现Instruction Page Fault的时刻，也就是进入trap_from_kernel的时候，发现两个核使用的都是一个内核栈，而这并不符合内核的设定。正常来说，内核运行的栈严格和当前处理器核运行进程的pid绑定，或者转入idle_cx也就是初始栈，但这仍然不可能出现两个核处在相同的栈。结合task_cx的问题，我们推测是栈的切换过程之中出现了空隙，使得这种异常现象得以发生。

最后，我们发现，是切换的时候锁的控制出现了空隙。在内核进行进程切换的时候，先将当前进程放入空闲进程队列，再从队列中寻找下一个进程，这使得该核在使用该进程栈的同时，却有可能将该进程放手给了另一个核。

因此我们必须强制使得每个核必须手握至少一个进程的锁，也就是先找到下一个可以切换的进程，再将当前进程放入队列中。但是显然，这会造成死锁，所以我们采用了非常巧妙的方法，那就是：如果当前没有下一个进程，就不进行进程切换，返回原来的进程。这样的让出争抢锁的方式避免了死锁的发生可能。

具体操作我们还遇到了一些小问题：
- cpu运行第一个程序的时候核自身没有程序，这个时候不能“返回原来的进程”，要特殊判断，持续争抢下一个进程。
- 注意手动释放锁
- loop块的作用域等等细节实现问题