## UltraOS优化

### trap优化

经过测试，内核syscall+sfence.vma 1000000次耗时8.6s，根据内核syscall0.4s，可以计算出sfence.vma耗时为8.2s(±0.2s)，而每次trap需要执行两次，而trap 24.2s，意味着这条指令是trap过程中最耗时的部分。为了将这条指令去掉，我们就得想办法不要人为清空TLB。

因此，我们选择将页表设置成不重叠，但是我们发现还是存在一些问题，因此进一步的合并了页表。

#### 信号机制优化

每次trap return的时候，我们都需要检测是否有待处理的信号。这个逻辑是通过遍历实现的，而现在我们通过直接的动态标志，也就是信号队列是否为空（rust会维护这个数据结构）的判定，来决定是否需要进行遍历。

> k210 before: 21us
>
> k210 after: 5.7us

这样的操作在k210上，大概提升了3.5倍性能。qemu上没有进行测试。


#### 非重叠地址空间

因此，我们将原有的内核和用户的虚拟地址完全分离开来，这样我们就可以在切换页表之后，不用刷新TLB了，也就是不需要使用sfence.vma指令了。但是我们发现，我们似乎只能在trap return的时候，将原有的sfence.vma去除，无法将trap in的时候的同样指令去除。我们认为页表切换还是需要通过sfence.vma进行相应的操作。因此我们决定进一步的合并内核和用户的页表。


> k210 before: 23.7us
> 
> qemu before: 60us
>
> k210 after: 2.5us
>
> qemu after: 2.3us

在k210上，我们大约提升了10倍，qemu上大概提升了25倍。

#### 合并页表

内核占有的虚拟地址空间为0X8000_0000-MAXVA以及MIMO(0X0C00_0000-0X5400_0000)，用户占有的虚拟地址空间为0X0000_0000-0X7FFF_FFFF。因此我们需要将两个地址空间完全合并。

其中对于内核非MIMO区域，在根页表（虚拟地址低30位位于次级页表，31-39位为根页表）中，均与用户虚拟地址隔离，因此我们设计所有的页表都共享该部分的地址，即将根页表对应的区域指向同一个二级页表。

其中还需要注意，虚拟地址的顶部有内核栈，这部分也不要忘记映射，我们选择取巧的和Trampoline一起映射。

而对于MIMO区域，因为我们使用了外部库，所以不能够更改地址，意味着用户和内核的地址在根页表上有交叉。所以我们不能采用根共享页表，需要进一步在第二级页表进行映射。以节省内存的消耗。

#### 用户地址不翻译（无需实现）

实际上，如果我们合并了页表，我们就可以直接翻译用户空间的地址，不必再软件模拟页表的查询，还要防止出现跨页的问题。但是为了实现这个，还需要做一些努力：

- 允许kernel访问用户地址：需要开启sstatus的SUM位，默认关闭，即页表项的U标志为只可用户态访问，开启后内核也可以访问。但是我们最好要用的时候开启，用完了就关闭，这样你好我也好。
- 不相信用户的地址
  - 用户传进来的指针很有可能是恶意攻击的指针，因此我们需要判定这个地址是不是在内核，以达到防止恶意篡改内核的目的，这也需要软件模拟的查询。
  - 同时，因为我们实现了lazy和CoW等机制，内核获得的地址可能是合法但是未映射的地址，因此需要预先的判定，这也需要软件模拟的查询。

这样看，我们直接翻译也没收到什么好处嘛，该模拟的还是得模拟，那我们干脆就不做了。


### 初始进程内存回收

在评测系统下，任何自定义的程序都必须要放入在OS程序中一同打包，我们采用的策略是先放置于内存中，然后初始化写入至外存，再exec initproc，initproc会exec shell。在写入外存之后，原来内存中存储程序的部分就可以空闲出来了，但是我们并没有回收。

我们的页帧分配器采用堆栈式进行管理，因此我们可以将单独对应空闲出来的页帧放入其中，以进行回收。

> Recycle memory: 8022e000-80258000 (2A000)
> 
> Kernel memory: 80200000-0x802ea000 (EA000) with 0.5MB heap
>
> Optimization: 42/234 = 17.95%
>
> Optimization: 42/106 = 39.62% (no heap)


### 程序载入的内存消耗

读取ELF文件，载入的过程:

1. 完整导入：需要将整个执行程序完整导入到内存，才开始解析。
2. Double-copy： 导入到内存后，会占用相当于整个程序大小的内存，之后，还会专门为了建立程序，复制进memory set之中，还会出现一次复制。

意味着我们载入一个程序，需要两倍的内存，一倍的堆。堆采用的架构为伙伴系统，而当前的busybox大小约为1.6MB，意味着我们需要申请1.6MB的堆。伙伴系统在处理该情况时，需要拿出最小2MB的连续空间，这意味着我们需要分配4MB的空间，但是内核只有6MB可用空间（8MB-RustSBI 2MB），这显然不能够满足我们的内存现有容量的限制。

#### 基本优化想法

1. 不完整导入就没有办法使用elf解析，各个字段都不明晰。对于ELF解析我也不懂。
2. 最多只能优化到 one copy zero heap，这样我们需要直接copy到“内存的剩余空间之中”（也就是页帧），而不能直接使用buffer（来自堆）储存，只要用了堆我们就必须再copy一次到“内存的剩余空间之中”。

第二点意味着，我们需要直接copy进内存剩余空间中。但是页帧的分配并非连续。不连续的分配就不能够在不使用页表的情况进行ELF解析。

但是只要我们优化了第一点，即如果我们能够拆分ELF解析，一小段一小段的解析，我们就可以解析一部分就分配一部分页帧，这样近似于 two copy zero heap。但是我们ELF解析是调用的库，因此我们需要重写对应的函数，这存在一定的难度，并且不能够复用，我们希望一一种更优雅的方式来解决这个问题。

对于two copy zero heap，还可以使用使用动态增长堆的做法。只要堆能够动态增长，就没必要区分堆和内存区域了。但是实际上并非如此，堆的增长容易，回收难。这会造成内存使用量的增多。

#### kmmap：内核虚拟化（采用）

内核虚拟化，具体指的是，在内核地址空间进行虚实地址非恒等映射，将原本不连续的空间通过地址映射，以连续的虚地址访问。

这样，我们就可以不必从堆中申请空间，而是拿出一块连续的空间进行相应程序的数据存放。这实际上是页帧分配和动态堆的折中，既避免了动态堆难以回收的困境，又避免了将内核完全虚拟化的额外负担。同时，我们的实现复用了mmap的思想，还为内核虚拟化功能提供了对应的支持，这是我们所认为最佳的方法。

最终我们的优化结果
> two copy one heap: >6.5MB
> 
> two copy zero heap: 4.38MB
>
> Optimization: 2.12/6.5 = 32.6%
>

我们在其中遇到了一些问题，发现页表项的U标志为仅用户可用的意思。因此内核虚拟化的时候不能设置该标志位，否则出发load/store page fault。（一个凶险的bug+愚蠢的脑袋）

#### Copy on Write: 优化fork之后占用的内存

__CoW__ 策略的具体方法是，当某个进程调用 __fork()__ 生成新的进程时，需要为新的进程创建一个新的地址空间 __MemorySet__ ，这个地址空间的内容应该是父进程的地址空间的完全复制，因此内容相同。在原来的情况下，这个过程会执行彻底的复制，但是由于子进程之后可能会马上就调用 __exec()__ 来载入新用户程序的elf文件，原来的内存彻底清除，就浪费了原来的复制时间。 __CoW__ 的做法是在创建新子进程的过程中，把子进程的虚拟页对应到父进程原有的物理页上，也就是父子公用同一个物理页。

这样的情况下，父子都只允许读这些空间而不能进行写操作，如果要进行写操作的话，写的那一方会为要写的那个虚拟地址页，自己申请分配一个新的物理页帧，在自己的页表中重新映射这个虚拟地址页，这时就可以独享单个物理页帧，也可以进行写操作。

对一个物理页帧的引用次数 __RefCount__ 决定了它是否是一个CoW策略的物理页。当一个新的进程的虚拟页映射了这个物理页帧的时候，它的 __RefCount+1__ ，当 __RefCount>1__ 时，就意味着不止一个进程引用了该物理页帧，也就是使用了CoW；当一个进程取消映射时， __RefCount-1__ 。

#### Lazy Stack & Heap: 动态分配的堆栈空间

在一开始的UltraOS中，我们采用预分配与映射的方式来为每一个用户程序预留一段固定大小的堆/栈空间，但实际上这段空间在大多数程序的运行过程中并未别完全使用，或是只是用了非常少的一部分。因此，随着堆的增长（brk）和栈的增长（push）来动态地增长，当需要使用到某个虚拟页的时候再给它分配物理页帧，可以节省足够的空间，以免在程序进行高强度的fork()的时候，可分配的物理内存空间不足。

但对于栈空间Stack来说，每个程序至少会用到一定大小的内存，这造成了每个程序栈空间的前几个一定会被分配。在Lazy的策略下，每次都要进入trap去给他们分配，实际上拖慢了系统的执行速度，我们参考Linux的方法，恰好给每个进程提前分配少量固定的堆栈空间，对于堆栈使用强度小的大部分程序，这些提前分配的空间恰好足够他们使用，不触发lazy alloc，对于少部分强度大的程序，后面分配的空间不一定全部都被使用，因此当用到时再触发trap来分配。

#### Lazy Mmap: mmap空间的节约策略

在busybox的测试中，我们频繁地遇到内存不足，物理页帧分配耗尽的情况，部分原因是测试中调用mmap对一个非常大（超出硬件总内存大小）的文件进行内存映射操作，但是实际上只使用了其中的一小部分进行读写。因此在mmap的过程中，很大一部分文件数据是不需要预先就映射到内存中的，同样采用和堆栈一样的lazy策略，仅通过trap对当前要进行读写的那个页进行页表映射和内存分配。

我们尝试了更加灵活的分配策略，当mmap映射的文件大小只有1个page的时候，大概率随后是要读取/写入这个page的，因此我们在mmap的时候对映射的大小进行判断，如果 __len<=PAGE_SIZE__ ，那么进行直接分配内存空间的映射，否则采用lazy策略。

#### 页换入换出（未实现）

既然我们没有办法压缩到我们需要的内存大小，那么索性就进行内存换入换出。将内存换入到外存中，以页为单位。

基本思想：当无空闲页帧时，抽取一个已用页帧，将其页表项进行标记（已被换出，不可访问），然后将其换入到外存特定的文件中，文件以专有的数据结构进行管理。当出发page fault时，需要增加换出判断，如果是其造成的，就应该要将对应的页进行换入（无空闲页帧又会造成换出）。

这种方法会有额外的功能实现，牵扯到文件系统和内存管理单元，工程量较大，因此我们目前不准备实现。


#### lazy exec + direct access（未实现，搁置）

事实上，我们现在用的ELF加载库需要整个加载进来。我们的想法是，直接解析头文件库。

因此我们需要自己编写解析ELF文件的逻辑，这样，我们仅需要加载ELF header（64bits），之后根据ELF头加载Program Header，据此可以获得Program所在的位置和大小。然后就此打住，不必再继续读取。解析所有的LOAD program（还要加上ELF和program header），然后使用mmap映射至应该在的虚拟地址。这样，我们的lazy exec就完成了。

之后，我们还提出了更加激进的方案。mmap可以直接绕过文件系统，通过预留的蔟号直接读取相应的数据。这提供了新的优化思路，也就是说，文件系统单独提供对于确定的簇号进行数据读取的旁路，以对特定的情况进行优化。这与open channel SSD的思想有一定的相似之处（liblightnvm直接访问device driver），可以提供多种访问SSD的方式。

现在我们的方法已经能够解决一定程度上的内存短缺问题，如果我们激进的使用lazy的办法，会影响到所有程序的执行速度。因此我们暂时不准备实现，以空间换时间（注意lazy在exec会更快）。